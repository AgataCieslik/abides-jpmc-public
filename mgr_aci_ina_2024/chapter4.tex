\chapter{Modelowanie przewagi informacji}
Większość modeli agentowych jest zbudowanych w zgodzie z dwoma założeniami, wynikającymi w dużej mierze z zaadaptowania metod zaczerpniętych z teorii aukcji (model IPV): 
\begin{enumerate}[I.]
\item \textbf{Równy dostęp do informacji}
\item \textbf{Niezależność i prywatność preferencji}
\end{enumerate}
W konsekwencji powyższych założeń, klasyczne modele agentowe rynku wykluczają wymianę informacji między agentami na temat stanu wiedzy i preferencji, znacząco przy tym upraszczając model i zawężając zbiór strategii graczy. Wyłączone również są przypadki, gdy wśród inwestorów dostęp do informacji jest zróżnicowany. 

Przenosząc powyższe założenia bezpośrednio na współczesne realia, możemy zauważyć, że istnieją przesłanki by je zakwestionować. W przeszłości głównym źródłem nierówności w wiedzy na temat wartości spółki był proceder tzw. \textit{insider tradingu} - wykorzystywania w handlu uprzywilejowanego dostępu do niepublicznej informacji. Współcześnie, w dobie zaawansowanych modeli predykcyjnych ceny, sytuacja nie jest tak jednoznaczna - przewaga informacji może wynikać z przewagi technologicznej. Również założenie niezależności i prywatności preferencji budzi oczywiste wątpliwości w kontekście odnotowywanych reakcji na publikowane w mediach społecznościowych rekomendacje czy też, coraz bardziej zauważalnych, prywatnych grup lub forów publikujących rekomendacje inwestycyjne. Z takimi motywacjami podejmujemy próbę uzupełnienia standardowego modelu o mechaniki zróżnicowania wiedzy między graczami oraz komunikacji między nimi. 

\section{Założenia}\label{sec:assums}
% powprowadzać parametry

Nie sposób ująć w jednym modelu wszystkie występujące zjawiska wynikające z nierównego dostępu do informacji i dzielenia się przekonaniami, zaczynamy więc od narzucenia ograniczeń na planowane rozszerzenie modelu.  

\subsection{Zdarzenie}
Zgodnie ze stylizowanymi faktami na temat rynku (Fakt \ref{fact:events}) zakładamy, że cena $p$ obowiązująca na rynku jest podatna na tzw. zdarzenia ekonomiczne, wiążące się z upublicznieniem danych wpływających na wartość instrumentu. 

Na potrzeby wprowadzenia prostego modelu przewagi informacji, założymy że w założonym zakresie czasowym symulacji $[t_0, T]$ dochodzi do zdarzenia $\eta = (t_{\eta},\Delta_{r})$, gdzie $t_{\eta}$ - czas zdarzenia, $\Delta_{r}$ - wynikająca z niego zmiana wartości fundamentalnej $r_t$ instrumentu. Dodatkowo przyjmujemy, że czas zdarzenia (publikacji) $t_{\eta}$ jest ustalony z wyprzedzeniem. 

Technicznie, powyższe założenia realizujemy modyfikując ciąg wartości fundamentalnych $r_t$ generowanych przez wyrocznię $O$ - zwiększając wartość $r_{t_{\eta}}$ o wartość $\Delta_{r}$ (w konsekwencji również kolejne $r_t$ dla $t>t_{\eta}$). Zdarzenie $\eta$ wprowadzamy jako parametr modelu. 

% rozważyć wrzucenie wykresu
\subsection{Informatorzy i obserwujący}
Zbiór klas agentów modelu rozszerzamy o dwa nowe typy agentów:
\begin{itemize}
\item \textit{InformedAgent} - informatorów, dysponujących wiedzą na temat nadchodzącego zdarzenia,
\item \textit{FollowerAgent} - obserwujących, postępujących zgodnie z rekomendacją daną przez obserwowanego informatora. 
\end{itemize}
Oba typy agentów częściowo bazują na klasach już istniejących w modelu (zgodnie ze  schematem \ref{fig:classdiagram}). 

Informator \textit{InformedAgent} bazuje na klasie \textit{ValueAgent} - agenta uzależniającego swoją strategię od obserwowanej wartości fundamentalnej. Informator dziedziczy po nim model obudzeń oparty na procesie Poissona oraz sposób estymacji finalnej wartości fundamentalnej ($r_T$ - wartości fundamentalnej na koniec założonego okresu czasu; sposób estymacji opisany szerzej w \cite{abides_explanation}). 

Z kolei obserwujący \textit{FollowerAgent} jest rozwinięciem agenta Zero Intelligence \textit{NoiseAgent}. Dziedziczy po nim model obudzeń oparty na rozkładzie U-kwadratowym oraz losowy rozmiar zlecenia zgodny z mieszanką rozkładów (daną tabelą \ref{tab:distmix}).

Wprowadzenie do modelu graczy informatorów i graczy obserwujących ma umożliwić modelowanie sytuacji, w której wokół osoby uprzywilejowanej dodatkową wiedzą skupia się grono obserwujących (np. subskrybentów płatnej grupy lub forum). Stąd, zakładamy, że obserwujący nie są zainteresowani preferencjami innych obserwujących. W konsekwencji graf komunikacji między obserwującymi a informatorem ma topologię gwiazdy. 
% rozważyć wrzucenie grafu-gwiazdy [lub grafu złożonego z kilku gwiazd]

\section{Uwzględnienie przewagi informacji}
Do celów uwzględnienia przewagi informacji wykorzystamy obiekt wartości prywatnych $\theta^i$ gracza (sekcja \ref{sec:preferences}), równocześnie wprowadzając ten element do modelu referencyjnego. Wartości $\theta^i$ wyznaczymy korzystając z założeń przyjętych w sekcji \ref{sec:assums}, a konkretnie ustalenia zdarzenia $\eta = (t_{\eta}, \Delta_r)$ jako parametru symulacji. W oparciu o dane zdarzenie $\eta$, wartości $\theta^i$ wyznaczamy jako próbkę z rozkładu o wartości oczekiwanej $\mu_{theta} = \Delta_r$ oraz wariancji $\sigma^2_{\theta}$ zależnej od wprowadzonego parametru niepewności oszacowania informatora. 

Uogólniając, różnicujemy przewagę informacji poprzez zróżnicowanie rozkładów wartości prywatnych agentów $F'_i$ (przede wszystkim ich wartości oczekiwanych).

\section{Komunikacja między agentami}

Komunikacja między obserwującymi agentami a informatorem odbywa się z wykorzystaniem mechanizmu zapytania. Agent obserwujący wysyła do informatora prośbę o udzielenie rekomendacji \textit{QuerySideRecommendation}. W odpowiedzi agent informator dodaje prośbę do kolejki oczekujących, rekomendację wyznaczając i wysyłając po pobraniu aktualnych cen przy kolejnym obudzeniu (Algorytm \ref{alg:informer}). 
\begin{pseudokod}
\caption{\textit{InformedAgent: onReceive}}\label{alg:informer}

\KwData{$m \in M$, $s_m \in [n],t_m \in [t_0, T], \xi \in \Xi$}
\vspace{0.5cm}
\If{$m = \text{\textit{QuerySideRecommendation}}$}{
pendingRequests.put(($t_m$, $s_m$))\tcp*[r]{kolejka oczekujących próśb}
}
\vspace{0.5cm}
\If{$\xi = \text{\textit{AWAITING\_SPREAD}}$}{
\If{$m = \text{\textit{QuerySpreadResponseMsg}}$}{
placeOrder()\tcp*[r]{agent realizuje swoje cele}
sendPendingRecommendations()\tcp*[r]{agent realizuje rekomendacje}
$\xi := \text{\textit{AWAITING\_WAKEUP}}$\;
}
}
\end{pseudokod}

\begin{pseudokod}
\caption{\textit{FollowerAgent: onReceive}}\label{alg:follower}

\KwData{$m \in M$, $s_m \in [n],t_m \in [t_0, T], \xi \in \Xi$}
\vspace{0.5cm}
\If{$\xi = \text{\textit{AWAITING\_SIGNAL}}$}{
\If{$m = \text{\textit{TradingSignal}}$}{
\eIf{$m=\text{\textit{BID}}$}{
$o_s := \textit{BID}$\tcp*[r]{interpretacja rekomendacji}
}{\If{$m=\text{\textit{ASK}}$}{
$o_s := \textit{ASK}$;\
}
}
sendMessage(\textit{QuerySpreadMsg});\\
$\xi := \text{\textit{AWAITING\_SPREAD}}$;\

}}
\vspace{0.5cm}
\If{$\xi = \text{\textit{AWAITING\_SPREAD}}$}{
\If{$m = \text{\textit{QuerySpreadResponseMsg}}$}{
placeOrder(side=$o_s$)\tcp*[r]{wykorzystanie rekomendacji}
$\xi := \text{\textit{AWAITING\_WAKEUP}}$;\\
}}

\end{pseudokod}
Rekomendacje informatora mają charakter dyskretny: zwracana jest wiadomość \textit{ASK}("SPRZEDAJ") lub \textit{BID} ("KUP") zgodnie z funkcją: 
$$
 R_t = 
 \left\{\begin{array}{lr}
        ASK, & \text{dla }p_t - \epsilon^I > \hat{r_t} +\theta^I_{\omega^I}\\
        BID, & \text{dla } p_t + \epsilon^I < \hat{r_t} +\theta^I_{\omega^I}\\
        NULL, & \text{w pozostałych przypadkach,}
        \end{array}

$$ 

gdzie:
\begin{itemize}
\item $R_t$ - rekomendacja w chwili $t$, 
\item $\hat{r_t} +\theta^I_{\omega^I}$ - aktualna wycena informatora, 
\item $\epsilon^I$ - margines niepewności przyjęty przez informatora.
\end{itemize}
Po otrzymaniu rekomendacji obserwujący agent wyznacza na jej podstawie stronę kolejnego zlecenia, a następnie wysyła zapytanie do agenta-giełdy o ceny kupna $a(t)$ i sprzedaży $b(t)$.
